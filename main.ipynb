{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from autocorrect import spell\n",
    "\n",
    "train_file = '/home/abhilash/Kaggle/Quora/Data/train.csv'\n",
    "glove_file = '/home/abhilash/MajorProject/MajorProject/glove.6B/glove.6B.300d.txt'\n",
    "glove_dim = 300\n",
    "batch_size = 32\n",
    "split_ratio = 0.99\n",
    "max_len = 0\n",
    "n_hidden = 128 \n",
    "n_classes = 2 \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_question(question):\n",
    "    question = str(question).lower()\n",
    "    question = question.strip('?/-().,:;')\n",
    "    question = question.strip(' ?')\n",
    "    question = re.sub(r'[^a-zA-Z0-9 ]','', question)\n",
    "    \n",
    "    clean_question = []\n",
    "    for i, ch in enumerate(question):\n",
    "        if i > 0 and ((question[i-1].isdigit() and question[i].isalpha()) or (question[i-1].isalpha() and question[i].isdigit())):\n",
    "            clean_question.append(' ')\n",
    "        clean_question.append(question[i])\n",
    "    processed_question = \"\".join(clean_question)\n",
    "    return processed_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unknown words not handled yet\n",
    "def generate_embeddings(word_dict, vocab_size):\n",
    "    embedding_matrix = np.random.uniform(low=-1, high=1, size=(vocab_size, glove_dim))\n",
    "    num_embeddings = 0\n",
    "    f = open(glove_file)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if(word in word_dict):\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix[word_dict[word]] = coefs\n",
    "            num_embeddings+=1\n",
    "    f.close()\n",
    "    print(\"Number of words \", vocab_size)\n",
    "    print(\"Number of embeddings \", num_embeddings)\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "count = 1 #0 for padding\n",
    "train_tuples = []\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "for (index, row) in train_df.iterrows():\n",
    "    q1 = clean_question(row['question1'])\n",
    "    q2 = clean_question(row['question2'])\n",
    "\n",
    "    train_tuples.append((q1, q2, row['is_duplicate']))\n",
    "\n",
    "    words1 = q1.split(' ')\n",
    "    words2 = q2.split(' ')\n",
    "\n",
    "    for word in words1:\n",
    "        if(word not in word_dict):\n",
    "            word_dict[word] = count\n",
    "            count += 1\n",
    "#             spelt_word = spell(word)\n",
    "#             if(word == spelt_word):\n",
    "#                 word_dict[word] = count\n",
    "#                 count += 1\n",
    "#             else:\n",
    "#                 word_dict[spelt_word] = count\n",
    "#                 count += 1\n",
    "    for word in words2:\n",
    "        if(word not in word_dict):\n",
    "            word_dict[word] = count\n",
    "            count += 1\n",
    "#             spelt_word = spell(word)\n",
    "#             if(word == spelt_word):\n",
    "#                 word_dict[word] = count\n",
    "#                 count+=1\n",
    "#             else:\n",
    "#                 word_dict[spelt_word] = count\n",
    "#                 count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = count+1\n",
    "embedding_matrix = generate_embeddings(word_dict, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tensor Flow Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.contrib import rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(data, word_dict):\n",
    "    \n",
    "    q1s = []\n",
    "    q2s = []\n",
    "    labels = []\n",
    "    for tup in data:\n",
    "        q1_seq = []\n",
    "        q2_seq = []\n",
    "        \n",
    "        q1_words = tup[0].split(' ')\n",
    "        for word in q1_words:\n",
    "            q1_seq.append(word_dict[word])\n",
    "            \n",
    "        q2_words = tup[1].split(' ')\n",
    "        for word in q2_words:\n",
    "            q2_seq.append(word_dict[word])\n",
    "        \n",
    "        q1s.append(q1_seq)\n",
    "        q2s.append(q2_seq)\n",
    "        labels.append(tup[2])\n",
    "    return q1s, q2s, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(size, batch_size, shuffle=False):\n",
    "    idx_list = np.arange(0, size, batch_size)\n",
    "    if(shuffle):\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + batch_size, size)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_data(seqs):\n",
    "    global max_len\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    \n",
    "    max_len = np.max(lengths)\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_batch_data(q1, q2, labels, batch_size):\n",
    "    minibatches = get_batches(len(q1), batch_size)\n",
    "    examples = []\n",
    "    for minibatch in minibatches:\n",
    "        m_q1 = [q1[t] for t in minibatch]\n",
    "        m_q2 = [q2[t] for t in minibatch]\n",
    "        l = []\n",
    "        for t in minibatch:\n",
    "            if(labels[t] == 0):\n",
    "                l.append([1, 0]) \n",
    "            else:\n",
    "                l.append([0, 1])        \n",
    "        m_q1 = pad_data(m_q1)\n",
    "        m_q2 = pad_data(m_q2)\n",
    "        \n",
    "        examples.append((m_q1, m_q2, l))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(q1, q2, labels, split_ratio):\n",
    "    l = len(q1)\n",
    "    train_len = int(l * split_ratio)\n",
    "    return q1[:train_len], q2[:train_len], labels[:train_len], q1[train_len:l], q2[train_len:l], labels[train_len:l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Could be optimized shuffling can be performed after vectorization\n",
    "def generate_data(): \n",
    "    random.shuffle(train_tuples)\n",
    "    q1, q2, labels = vectorize(train_tuples, word_dict)\n",
    "    q1_train, q2_train, labels_train, q1_val, q2_val, labels_val = split_data(q1, q2, labels, split_ratio)\n",
    "    train_data = gen_batch_data(q1_train, q2_train, labels_train, batch_size)\n",
    "    val_data = gen_batch_data(q1_val, q2_val, labels_val, batch_size)\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.constant(0.0, shape=[embedding_matrix.shape[0], embedding_matrix.shape[1]], dtype=tf.float64), trainable=True, name=\"W\", dtype=tf.float64)\n",
    "embedding_placeholder = tf.placeholder(tf.float64, [embedding_matrix.shape[0], embedding_matrix.shape[1]])\n",
    "embedding_init = W.assign(embedding_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_steps = 28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = tf.placeholder('int32', [batch_size, None])\n",
    "x2 = tf.placeholder('int32', [batch_size, None])\n",
    "\n",
    "\n",
    "y_train = tf.placeholder('int32', [batch_size, 2])\n",
    "\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([4*n_hidden, n_classes], dtype=tf.float64))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes], dtype=tf.float64))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BiRNN(x1_emb, x2_emb, weights, biases):\n",
    "\n",
    "    print(len(x1_emb), len(x2_emb))\n",
    "    print(x1_emb[0].shape, x2_emb[0].shape)\n",
    "    with tf.variable_scope('forward1'):\n",
    "        lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    with tf.variable_scope('backward1'):\n",
    "        lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    with tf.variable_scope('op1'):\n",
    "        outputs1, _f, _w = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x1_emb, dtype=tf.float64)\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('forward2'):\n",
    "        lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    with tf.variable_scope('backward2'):\n",
    "        lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    with tf.variable_scope('op2'):\n",
    "        outputs2, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x2_emb, dtype=tf.float64)\n",
    "    \n",
    "    outputs1 = tf.reverse(outputs1, [1])\n",
    "    outputs2 = tf.reverse(outputs2, [1])\n",
    "    \n",
    "    q1_final_emb = tf.reshape(tf.slice(outputs1, [0, 0, 0], [-1, 1, -1]), [batch_size, 2*n_hidden])\n",
    "    q2_final_emb = tf.reshape(tf.slice(outputs2, [0, 0, 0], [-1, 1, -1]), [batch_size, 2*n_hidden])\n",
    "    \n",
    "    return tf.matmul(tf.concat([q1_final_emb, q2_final_emb], 1), weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1_embedding = tf.nn.embedding_lookup(W, x1)\n",
    "x2_embedding = tf.nn.embedding_lookup(W, x2)\n",
    "\n",
    "pred = BiRNN(tf.unstack(x1_embedding, axis=0), tf.unstack(x2_embedding, axis=0), weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_train))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y_train,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding_matrix})\n",
    "    \n",
    "    epoch = 1\n",
    "    while epoch < 3:\n",
    "        train_data, val_data = generate_data()\n",
    "        ct=1\n",
    "        for tup in train_data:\n",
    "            sess.run(optimizer, feed_dict={x1:tup[0], x2:tup[1], y_train:tup[2]})\n",
    "            ac = sess.run(accuracy, feed_dict={x1:tup[0], x2:tup[1], y_train:tup[2]})\n",
    "            print(\"In ct \" + str(ct) + \" Training Accuracy is \" + \"{:.6f}\".format(ac) )\n",
    "            if(ct%10 == 0):\n",
    "                val_acc = tf.reduce_mean([sess.run(accuracy, feed_dict={x1:val_tup[0], x2:val_tup[1], y_train:val_tup[2]}) for val_tup in val_data if(len(val_tup[0])==32)])\n",
    "                print(val_acc.eval())\n",
    "#                 print(\"In Epoch \" + str(epoch) + \" Validation Accuracy is \" + \"{:.6f}\".format(val_acc) )\n",
    "            ct+=1\n",
    "        epoch += 1\n",
    "    print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
